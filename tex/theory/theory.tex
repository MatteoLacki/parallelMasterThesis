%!TEX root = <parallelTemperingMaster.tex>
\section*{Problem statement}

Let $(\Omega, \mathfrak{F})$ be a measurable space. $\Omega$ --- the \sspace, a subset of a Polish space, and $\mathfrak{F}$ - countably-generated Borel subsets. Finally, denote the unit interval by $\mathcal{I} = [0,1]$. 

\begin{Problem}
	\item{\label{Problem}
		Given a distribution $\Pi: \mathfrak{F} \mapsto \mathcal{I}$
		\begin{Problem}
		 	\item absolutely continuous with respect to some measure on $\Omega$, with a density $\pi$ that we know to evaluate 
		 	\item density $\pi$ known up to its proportionality factor
		 	\item \label{multiModality}density $\pi$ being multimodal
		\end{Problem} 
		draw a sample $\{ X_1, \dots, X_\nn \}$ from $\Pi$ so that for any function $g \in \mathbb{L}^1(\Pi)$ one can approximate well the integrals with empirical means
		$$\int_\Omega g \,\dip \approx \frac{1}{\nn}\sum_{i = 1}^\nn g(X^{[i]}). $$ 
	}
\end{Problem}

The rather abstract setting might seem to be an overshot, because, as the reader suspects, the most obvious and natural choice would be that of $\Omega = \real^\nn$ and $\mathfrak{F}$ the corresponding Borel sets. That, however, is not always the case --- for very often one encounters some discrete spaces as well. Moreover, the abstract setting underlines the modularity of the implementation of the algorithm itself, being described in CHAPTER.

Had it been that $\pi$ was not multimodal then by using the usual Metropolis-Hasting algorithm one could have easily solved the \ref{Problem}. Adding condition \ref{multiModality}, however, makes the use of that algorithm problematic because of its localness --- a phenomenon well visualised in CHAPTER.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{The \PT} 
The Parallel Tempering algorithm is based on the idea of extending our attention to the product space $(\Omega^\lll, \mathfrak{F}^{\otimes \lll}, \pi_\beta)$ equiped with a specific Markov chain\footnote{Here $\mathfrak{F}^{\otimes \lll} \equiv \underbrace{\mathfrak{F} \otimes \dots \otimes \mathfrak{F}}_{\text{L times}}$ is the usual product sigma algebra.}. The main idea is that on each copy of $\Omega$ the corresponding chain will encounter a slightly modified version of the density of interest $\pi$. In the usual \PT\, one settles for 

\begin{assumptions}
	\item $\pi_\beta \propto \pi^{\beta_1} \times \dots \times \pi^{\beta_\lll},$\label{product form}
\end{assumptions}	

where $\beta = (\beta_1 , \dots , \beta_\lll)$ are such that $1 = \beta_1 > \dots > \beta_\lll > 0$. Their inversions, $T_i = \frac{1}{\beta_i}$, called temperatures, do form therefore a growing sequence of numbers. This naming convention finds its origin in the use of the \PT\, in Statistical Physics where the $T_i$ parameters had direct interpretation. 

We do stress that the normalising constants for each $\pi^{\beta_l}$ are not assumed to be known, as we might not even know it for $\pi$ itself.

A Markov Chain\footnote{For the algorithm step numbering we shall henceforth use the square brackets notation.} $X \equiv \{ X^{[k]}\}_{k \geq 0}$ is then constructed on $\Omega^L$. Its construction makes explicit use of two kernels that make direct reference to $\pi^\beta_l$ --- first one, being a standard \randomWalk, is responsible for the exploration of the \sspace. The other, called the \swapStep, takes care of communication between different subchains ---  for $X$ can also be thought of as being a collection of coordinate subchains 
$$X^{[k]} = (X_1^{[k]}, \dots, X_\lll^{[k]}).$$
Observe that the first subchain corresponds to the solution of the \ref{Problem}.

The main idea behind the construction of chain $X$ is that the subchains operating on tempered versions of $\pi$ influence the subchains linked with lower temperatures by offering them their own accepted proposals. This should be highly beneficial, as more tempered chains will encounter less problems in accepting proposals from regions where the first chain would hardly ever stray.  

To make this approach work at all we are still bound to make some other assumptions. 

\begin{assumptions}[resume]
	\item The starting points $X^{[0]}$ are selected so that $\pi(X_l^{[0]}) > 0$ for $l \in \{1,\dots,\lll\}$.\label{beginAtFullProbabilitySet}
\end{assumptions}
	
Obviously, what follows is that $\pi(X_l^{[0]})^\beta_k > 0$ as well. It is important, for otherwise, what will be demonstrated in the following sections, the algorithm would be dividing by $0$ and break. Moreover, what follows from \ref{beginAtFullProbabilitySet} is that the algorithm will never find itself in a point on which the density evaluates to zero.  

The choice of modifying $\pi$ by exponentiation, as in \ref{product form}, besides its simplicity\footnote{For in general one could settle for some other kind of homotopic deformation.}, has considerable advantages in the analysis of the algorithm. For in the \randomWalk\,phase of the algorithm's step suppose a proposal $y$ is generated by the $l^\text{th}$ chain from a region with less $\Pi$ probability on it in comparison to the position $x$ occupied in the last step, so that $\pi(y) < \pi(x)$. What follows,

$$ 
	\alpha_{\beta_l}(x,y) =  1 \wedge \Biggl(\frac{\pi(y)}{\pi(x)}\Biggl)^{\beta_l} = 
	\Biggl(\frac{\pi(y)}{\pi(x)}\Biggl)^{\beta_l} > \frac{\pi(y)}{\pi(x)} = 1 \wedge \frac{\pi(y)}{\pi(x)} = \alpha_{\beta_1}(x,y),
$$

which means that the probability of accepting such a proposal is larger on the more tempered chains. On the other hand, if  $\pi(y) > \pi(x)$, then this probability remains the same, $\alpha_{\beta_1}(x,y) = \alpha_{\beta_k}(x,y)$. This assures that more tempered chains can make longer excursions into regions not so much $\Pi$-probable. 

One cannot neglect one possibility though: if the support of $\pi$ is not connected and the connected components are far away then it is highly improbable that the algorithm will give unbiased results because of poor mixing that cannot be fixed by exponentiation and some other technique should be used. We stress this point because in computer simulations one might tumble upon this problem because of the finite arithmetic --- the rounding of numbers to zero.    

As pointed out before, the update mechanism of the \PT\,consists of two stages. Assume that the algorithm has reached $n^\text{th}$ step, $X^{[n-1]}$. Following notation used in \citet*{BM1}, we introduce two kernels that act subsequently on $X^{[n-1]}$

$$X^{[n-1]} \overset{\metro}{\rightarrow} \widetilde{X}^{[n-1]} \overset{\swap}{\rightarrow} X^{[n]}.$$

We shall refer to $\swap$ and $\metro$ as to the swap kernel and the random-walk kernel respectively\footnote{$M$ as Metropolis.}. These will be now presented in greater detail.



	\subsubsection*{The \randomWalk\, Kernel}
	
Take a point in the \sspace, $x = (x_1, \dots, x_\lll) \in \Omega^L$. To assign probabilities to the proposal of the next step, given $x$, we consider events $A_i \in \mathfrak{F}$. We shall assume that 

\begin{assumptions}[resume]
	\item{
		\randomWalk\, proposal generation occurs independently on all subchains 	
		$$\metro (x, A_1 \times \dots \times A_L) = \prod_{l=1}^L \metrobis{l}(x_l, A_l).$$
	}
\end{assumptions}

The $\metrobis{l}(x_l, A_l)$ corresponds to standard Metropolis kernel, as in \citet*{CharlesJ.Geyer}. To be more precise

\begin{equation}\label{metropolisKernel}
	\metrobis{l}(x_l, A_l) \equiv \int_A \alpha_{\beta_l} (x_l, y_l) q_{\Sigma_l} (y_l - x_l) \mathrm{d }y_l + \iii(x_l,A) r(x_l) 	
\end{equation}

where 
$$r(x_l) = \int [1 - \alpha_{\beta_l} (x_l, y_l)] q_{\Sigma_l} (y_l - x_l) \mathrm{d }y_l$$
is the probability of not accepting the proposal and the subprobability density of accepting the update \footnote{Also known as the Hastings ratio.} is

\begin{equation}\label{acceptance prob}
	\alpha_{\beta_l} (x_l, y_l)  \equiv 1 \wedge \frac{\pi^{\beta_l}(y_l)}{\pi^{\beta_l}(x_l)}.
\end{equation}

To understand why $\metrobis{l}$ is given by Eq. \ref{metropolisKernel} we note that the role of the kernel is to assign probability to the next step of the algorithm given that the previous one assumed value $x$\footnote{So we abstract here from how the actual value $x$ was obtained, being mysteriously revailed to us by Nature, under the guise of random number generator. In fact kernels and their duals play principal role in the Bayesian statistics.}. The \MH\, goes as follows\footnote{We omit the inverse temperatures for time being.}: 


\begin{Algo}[Metropolis-Hastings]
	Initial state $X^{[0]}$ s.t. $\pi(X^{[0]}) \not=0$
	
	\begin{algorithmic}
		\Repeat 
			\State Suppose $X^{[n]} = x$.
			\State Draw proposal $Y \sim q(x,y)\mathrm{d}\,y$
			\State Draw $U \sim \mathcal{U}(0,1)$
			\If{$U \leq \alpha(x,Y)$}
				\State $X^{[n+1]} = Y$	
			\Else
				\State $X^{[n+1]} = X^{[n]}$	
			\EndIf
		\Until{Number of steps reached.}	 
	\end{algorithmic}		
\end{Algo}

Therefore, one should assign probability to event $\{ X^{[n+1]} \in A\}$ for all $A \in \mathfrak{F}$. But this is done, as

$$\int_A  \alpha(x,y) q(x,y) \mathrm{d}\,y = \int_A \prob \Big( U \leq \alpha(x,y) \Big)q(x,y) \mathrm{d}\,y =$$
$$
	=\int_A \prob\Big( U \leq \alpha(x,Y) | Y=y \Big)q(x,y) \mathrm{d}\,y = \prob\Big( U \leq \alpha(x,Y), Y \in A \Big)=
$$
$$
	= \prob\Big(X^{[n+1]} \not= X^{[n]} , X^{[n+1]} \in A \Big)
$$

where we assume $Y$ to be drawn out of the proposal distribution and $U$ to be independent of it. Setting $A = \Omega$ one notices that\footnote{Under the sensible assumption that the proposal distribution does not have any atom in $x$ --- this is usually automatically assumed by considering a distribution that is absolutely continous w.r. to Lebesgue measure.} 

$$ \prob\Big(X^{[n+1]} = X^{[n]}\Big) = \int  [ 1 - \alpha(x,y)] q(x,y) \mathrm{d}\,y = r(x)$$
--- precisely as claimed. 

In the above equations the proposal distribution being used, following notation from \cite{CharlesJ.Geyer}, is $q(x_l,y_l) = q_{\Sigma_l} (y_l - x_l)$, where $q_{\Sigma_l}$ is the density of $\mathcal{N}(0, \Sigma_l)$. 

The simple form of Eq. \ref{acceptance prob} stems from proposal distribution symmetry, $$q(x_l,y_l) = q(y_l,x_l).$$

The choice of $\alpha$ is motivated by assuring the self-adjointness of the $\metro$ operator, which in turn implies that distribution $\Pi$ is preserved, so that
	
\begin{equation}
	\Pi \metro = \Pi.		
\end{equation}	

As indicated by \cite{BM2}, the implementation of $\metro$ consists of separate simulation of $\metrobis{l}$ for each of the coordinate chains,  $X^{[n-1]}_l$.

	\subsection*{The swap kernels}

Now we address the question of how to interlace the independent chains generated with kernel $\metro$. We want to swap the generated states so that chains with higher temperature influence the regions of lower temperatures. Being more precise, the swapping gives some possibility to the chains with lower temperatures to find themeselves in states that would otherwise be very unlikely to happen, the path leading to them being very unlikely to occur - these paths are rendered more probable for the chains with higher temperature. The swaps are made at random so that the probability of swaps takes into account some properties of $\pi$ evaluated on the last points from all the chains. 

Before passing to detailed description of different ways the swaps might be made, which we call \strats, let us first describe the properties of the swap kernel $\swap$ common to all \strats. In all cases, we rely on the following assumption

\begin{assumptions}[resume]
	\item At one step of the algorithm, there will be only one possible swap between a chosen at random pair of coordinates.
\end{assumptions}

The restriction here is not so much in the number of swaps per step. What seems to be more relevant is rather the constraint to a particular subset of the group of all permutations --- for one could certainly envisage that any random permutation could take place. However, it simplifies the interpretation of \strats\, that are to be described. 

Let us denote the swap operator by $S_{ij}$, so that 
$$S_{ij} x = (x_1, \dots, x_{i-1}, x_j, x_{i+1}, \dots, x_{j-1}, x_i, x_{j+1}, \dots, x_L).$$ 
We require $\swap(x, \circ )$ to be a measure concentrated on the set of all possible pairs of swaps between coordinates of $x = (x_1, \dots, x_L)$, i.e. on the set $\mathfrak{S}_x \equiv \{ S_{ij}x : i < j  \}$. We note that the proposal measure for the swap kernel $\mathcal{Q}$ is of the following form 

\begin{equation*}
	\mathcal{Q}(x, A) \equiv \underset{i < j}{\sum} p_{ij}(x) \mathbb{I}_A (S_{ij} x)
\end{equation*}	 

Then, the swap kernel would be of the following form, for any $x \in \Omega^L$ and $A$

\begin{equation*}
	\swap(x,A) = \int_{A} \alpha_\text{swap} (x,y) \mathcal{Q}(x, \mathrm{d}\,y) + r(x) \mathcal{I}(x,A)
\end{equation*}	

where $r(x) = 1 - \underset{\Omega^L}{\int} \alpha_\text{swap} (x,y) \mathcal{Q}(x, \mathrm{d}\,y) $. Plugging $\mathcal{Q}$ permits us to write

\begin{align*}
	\begin{split}
		\int_{A} \alpha_\text{swap} (x,y) \mathcal{Q}(x, \mathrm{d}\,y) &= \underset{ i < j}{\sum} p_{ij}(x) \int \alpha_\text{swap} (x,y) \mathbb{I}_A (y) \delta_{S_{ij}x}(\mathrm{d}\, y) \\ &= \underset{ i < j}{\sum} p_{ij}(x) \alpha_\text{swap} (x, S_{ij}x) \mathbb{I}_A(S_{ij} x),
	\end{split}
\end{align*}	

so that finally

\begin{equation*}
	\swap(x,A) = \underset{ i < j}{\sum} p_{ij}(x) \alpha_\text{swap} (x, S_{ij}x) \mathbb{I}_A(S_{ij} x) + \Big( 1 - \underset{ i < j}{\sum} p_{ij}(x) \alpha_\text{swap} (x, S_{ij}x)\Big) \mathcal{I}(x,A),
\end{equation*}	

where 

	$$\alpha_\text{swap}(x,y) = \frac{\pi_\beta( \mathrm{d}\, y ) \mathcal{Q}(y, \mathrm{d}\,x)}{\pi_\beta( \mathrm{d}\, x ) \mathcal{Q}(x, \mathrm{d}\,y)} \wedge 1$$

is the Radon-Nikodym derivative of two measures. It's calculated with respect to measure $ \mu (\mathrm{d}\,x, \mathrm{d}\,y) \equiv \pi(\mathrm{d}\,x) \mathcal{Q}(x, \mathrm{d}\,y)$. The measure that is being differentiated is $\mu$ composed with the exchange coordinates operation, $R(x,y) = (y,x)$\footnote{Which is obviously measurable in the appropriate sense.}. So finally $\alpha_\text{swap} \equiv \frac{\mathrm{d}\, \mu \circ R^{-1}}{\mathrm{d}\, \mu}$.

Observe that thanks to the proposal's support finiteness we actually get 

	$$\alpha_\text{swap}(x,y) =  \frac{\pi_\beta( y ) \mathcal{Q}(y, x)}{\pi_\beta( x ) \mathcal{Q}(x, y)} \wedge 1.$$


Now, since $y \in \mathfrak{S}_x $, the tagetted $\pi_\beta$ is a tensor of measures, $Q(x, S_{ij} x) = p_{ij}(x)$, and $Q( S_{ij} x, x) = p_{ij}(S_{ij}x)$ \footnote{Here we pass from measure notation to probability function notation, $\mathcal{Q}(x, \{S_{ij}x \}) = \mathcal{Q}(x, S_{ij}x)$. We can do it because $\mathcal{Q}$ is purely atomic given $x$.}, we get finally 

\begin{equation*}
	\alpha_\text{swap}(x,S_{ij} x) = \Big[  \Big(\frac{\pi(x_j)}{\pi(x_i)} \Big)^{\beta_i - \beta_j}  \frac{ p_{ij}(S_{ij} x )}{ p_{ij}( x ) }\Big] \wedge 1
\end{equation*}	

In our computer simulations several swapping strategies have been tested. For instance,

\begin{strategy}
	\item 
		$
			p_{ij}(x) \propto 
			\frac{\pi (x_j)}{\pi( x_i )} \wedge \frac{\pi (x_i)}{\pi( x_j )} = 
			\exp \Big( - \big| \log ( \pi(x_j) ) - \log ( \pi(x_i) ) \big| \Big).
		$\label{strat1} 
\end{strategy}

In this particular the proportionality is in fact equality, because the above expression is transposition symmetric and this considerably simplifies the statistical sum analysis. In general, we want $p_{ij}(x) \propto h(x_i,x_j)$ for some function $h$. Thus,

\begin{equation*}
	\frac{p_{kl}(S_{kl}x)}{p_{kl}(x)} = \frac{\frac{h(x_l,x_k)}{A(\tilde{x}_{kl})+f(x_k)+g(x_l)+h(x_l,x_k)}}{
	\frac{h(x_k,x_l)}{A(\tilde{x}_{kl})+f(x_l)+g(x_k)+h(x_k,x_l)}
	} = 
	\underbrace{\frac{h(x_l,x_k)}{h(x_k,x_l)}}_\text{'direct effect'} 
	\times 
	\underbrace{\frac{A(\tilde{x}_{kl})+f(x_l)+g(x_k)+h(x_k,x_l)}{A(\tilde{x}_{kl})+f(x_k)+g(x_l)+h(x_l,x_k)}}_\text{'distribution effect'},	 	 	
\end{equation*}  
where $\tilde{x}_{kl}$ is $x$ without its $k^\text{th}$ and $l^\text{th}$ coordinates, $f(x_l) = \sum_{i<l, i\not=k} h(x_i, x_l)$, and $g(x_k) = \sum_{j>k, i\not=l}h(x_k, x_j)$. As we see it is fairly straightforward to compare the above expression to $1$ if the 'distribution effect' equals $1$, and difficult had it been the other case. To assure this happens, one can settle for a function $h$ that is transposition invariant, $h(x,y)=h(y,x)$. This is certainly the case of \ref{strat1}. In this strategy therefore we are promoting exactly swaps between coordinates whose proposals have relatively the same $\pi$ density values, i.e. $\pi (x_j) \approx \pi (x_i)$. 



\begin{strategy}[resume]
	\item 
		$
			p_{ij}(x) \propto 
			\frac{\pi (x_j)}{\pi (x_i)} \wedge 1 = 
			\exp \Big( - \big[ \log ( \pi(x_j) ) - \log ( \pi(x_j) ) \big]\Big) \wedge 1.$\label{strat2}
\end{strategy}

This strategy breaks the symmetry of the previous one rendering the evaluation of the effect on the $\alpha_\text{swap}$ challenging, if not impossible. The main idea behind this strategy was to  


The subsequent strategies are again functions of the fist strategy.

\begin{strategy}[resume]
	\item 
		$p_{ij}(x) \propto 
			\Big( \frac{\pi (x_j)}{\pi( x_i )} \wedge 
			\frac{\pi (	x_i)}{\pi( x_j )} \Big)^{|\beta_i - \beta_j|} = 
		\exp \Big( - |\beta_i - \beta_j| \times \big| \log ( \pi(x_j) ) - \log ( \pi(x_i) ) \big| \Big).$\label{strat3} 
\end{strategy}

This strategy permits us to soften a bit the requirement that $\pi(x_j) \approx \pi (x_i)$. This effect is strengthened for coordinates that are similarly tempered, i.e. where $\beta_i - \beta_j \approx 0$. Swaps between adjacent chains will be therefore more probable. 

One can also use a strategy that incorporates some distance measure between points, $\rho$ being any quasi-metric. \ref{strat4} is a good example of such procedure - the more distant the points, the less probable it is for them to get swapped.  

\begin{strategy}[resume]
	\item 
		$p_{ij}(x) \propto \Big( \frac{\pi (x_j)}{\pi( x_i )} \wedge \frac{\pi (x_i)}{\pi( x_j )} \Big)^\frac{|\beta_i - \beta_j|}{1 + \rho(x_i, x_j)} = \exp \Big( - \frac{|\beta_i - \beta_j| \times | \log ( \pi(x_j) ) - \log ( \pi(x_i) ) |}{{1 + \rho(x_i, x_j)}} \Big).$\label{strat4}
\end{strategy} 

Finally, for purposes of comparisons with \citet{BaragattiParallelTemperingWithEquiEnergyMoves}, we include also strategies that are \sspace\, independent, i.e. the probability of swaps does not depend on the evaluations of the density $\pi$ in the sample points. Such na\"ive strategies include 

\begin{strategy}[resume]
	\item $p_{ij} = \frac{2}{\lll (\lll - 1)}$\label{strat5}
\end{strategy}

and 

\begin{strategy}[resume]
	\item $p_{ij} = \frac{1}{\lll - 1} \ind{\{|i-j|=1\}}$\label{strat6}
\end{strategy}

amounting to choosing uniformly among all possible swaps and all neighbouring-in-temperature swap respectively. 