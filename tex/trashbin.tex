	\subsection*{Metropolis-Hastings-Green algorithm.}

The ideas behind the random-walk kernel were straightforward. Before passing to the detailed description of the swap kernel, we give here a brief account of how the MHG algorithm works. The description together with the notation are taken from \cite{CharlesJ.Geyer}, pages 79-81. The introduction of this notation will be of considerable help in the understanding of the next section. 

The algorithm aims at generating samples from any distribution $\rho$ on measurable space,

	$$(\Gamma, \mathfrak{G}),$$

where $\Gamma \subset \mathbb{R}^n$ and $\mathfrak{G}$ is the appropriate borel $\sigma$-algebra. We suppose that we know $\rho$ up to a constant, so that $\rho = \frac{\eta}{\eta(\omega)}$, where $\eta$ is any finite real measure \footnote{In order to avoid confusion, in this section we use different notation for spaces and distributions than it was the case before. In the forthcoming applications, we will take $\Gamma$ to be $\Omega^L$, $\rho$ equal to $\pi_\beta$ and so on \dots}, 

	$$\rho: \mathfrak{G} \mapsto \mathbb{R}_{+}.$$  

To generalise the notion of Hasting's ratio, $R(x,y)$, one introduces first a suitable measure $\mu$ on the measurable space 

	$$(\Gamma^2, \mathfrak{G}^{\otimes 2}),$$ 

precisely where we assume our current-state $x$ and proposal $y$ to live together. Since our understanding of $y$ is that it is some value conditioned on the current-state $x$, the natural choice for $\mu$ is that of a regular conditional measure

	$$\mu(\mathrm{d }x, \mathrm{d }y)   \equiv \eta (\mathrm{d} x) Q(x, \mathrm{d } y),$$

where $Q(x,A)$ is our proposal kernel, possibly substochastic.
 
 
We also need a coordinate-swapping mapping $\phi: \Gamma^2 \mapsto \Gamma^2$ given by $$\phi(x,y) = (y,x),$$ which is obviously measurable, being continuous. Our understanding of the Green's ratio is now that of

	$$R(x,y) \equiv	\frac{\mathrm{d }(\mu \circ \phi^{-1})}{\mathrm{d }\mu} (x,y) = \frac{\rho(\mathrm{d y} )Q(y, \mathrm{d }x)}{\rho(\mathrm{d }x) Q(x, \mathrm{d }y)},$$

where $\phi^{-1}$ is the counter-image function related to $\phi$ and the derivative is understood in the Radon-Nikodym sense. 

Since we want to use the MHG algorithm with the state-dependent mixing, we shall equip both $Q$ and $R$ with additional indices $k$ that belong to $\mathcal{K}$, a set\footnote{Which can be infinite. In that case the forthcoming sums should be understood as integrals with respect to the counting measure.}.  

We require that

\begin{itemize}
	\item $Q_k (x,S)$ is known for all $k$,
	\item $\forall_{x \in \Gamma} \underset{ k \in \mathcal{K}}{\sum} Q_k (x,S) \leq 1$,
	\item For all $k \in \mathcal{K}$ $$R_k (x,y_k) \equiv \frac{\rho(\mathrm{d } y_k)Q_k (y_k, \mathrm{d }x)}{\rho(\mathrm{d } x)Q_k (x, \mathrm{d }y_k)}$$ is known ans possible to evaluate for any $x$ and $y_k$ \footnote{We introduce an extra index in $y_k$ to underline the potential difference between proposals under different kernels $Q_k$.},
	\item For each $x$ and $k$ its possible to draw from $$P_k(x, \circ) \equiv \frac{Q_k (x, \circ)}{Q_k (x, \Gamma)}$$
\end{itemize}

If these requirements are fulfilled, then the MHG update goes as follows

\begin{algorithm}
\item Simulate random index $K$ with probability $\mathbb{P}( K = k ) = Q_k (x,\Gamma)$.
\item[] With probability $1-\underset{k \in \mathcal{K}}{\sum} Q_k (x,\Gamma)$ skip the remaining steps and stay at $x$.
\item Simulate $Y \sim P_K (x, \circ)$.
\item Calculate $R_K (x,Y)$.
\item Accept $Y$ with probability $1\wedge R_K (x,Y)$.
\end{algorithm}

The kernel of the MHG update is of the following form

	$$P(x,A) \equiv \underset{k \in \mathcal{K}}{\sum} \int_A \alpha_k (x,y_k) Q_k(x, \mathrm{d }y_k) + \delta_x (A) \Big(1 - \underset{k \in \mathcal{K}}{\sum} \int_{\Omega} \alpha(x,y_k) Q_k(x,\mathrm{d }y_k) \Big), $$

where $\alpha(x,y_k) \equiv 1\wedge R_K (x,y_k)$. Note that this kernel is stochastic. According to the general theory, as stated in \cite{CharlesJ.Geyer}, the MHG update is reversible with respect to $\rho$. 






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{tabular*}{\textwidth}{cc}		
	 	
% \begin{minipage}[b]{0.5\textwidth}
% \centering
% 	\begin{lstlisting}
% 	initialize = function(
% 		iterationsNo 	= NULL,
% 		burnIn 			= 2000L,
% 		...
% 	){	
% 	\end{lstlisting} 
% \end{minipage} &

% \begin{minipage}[b]{0.5\textwidth}
% 	This method initialises the \algo\,object. The default setting of the iterations number to \textsc{NULL} is technical and bypasses an error in the Reference Classes implementation: the Reference Classes objects have their constructors that are again Reference Classes objects. Calling the constructor automatically calls generation of the underlying object. This cannot be constructed properly without user-provided inputs. An easy solution is to include in that objects an if statement that checks whether what happens is not the above-mentioned case and construct an empty structure if that is so.
% \end{minipage}
 
% \end{tabular*}		 

% \begin{tabular}{p{4cm}p{5cm}}		
	 	
% \begin{minipage}[b]{0.5\textwidth}
% \centering
% 	\begin{lstlisting}
% 	initialize = function(
% 		iterationsNo 	= NULL,
% 		burnIn 			= 2000L,
% 		...
% 	)	
% 	\end{lstlisting} 
% \end{minipage} &

% \begin{minipage}[b]{0.5\textwidth}
% 	This method initialises the \algo\,object. The default setting of the iterations number to \textsc{NULL} is technical and bypasses an error in the Reference Classes implementation: the Reference Classes objects have their constructors that are again Reference Classes objects. Calling the constructor automatically calls generation of the underlying object. This cannot be constructed properly without user-provided inputs. An easy solution is to include in that objects an if statement that checks whether what happens is not the above-mentioned case and construct an empty structure if that is so.
% \end{minipage}
 
% \end{tabular}		 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{minipage}[h]{\linewidth}
\begin{algorithm}
	\item Let $KS := 0$, $\jj = \nn+1$. 
	\item Read $\alpha$
	\item Prepare $\Phi_x$ and $\Phi_y$.
	\item While $i \in \{1, \dots, \nn+1\}$, and while $j \in \{ 1, \dots, \ii \}$ 
	\begin{algorithm}
		\item $B_{ij} := 
			\begin{cases} 
				B_{i-1,j-1} + \text{adequate charge}, \text{ if $R_{ij}$'s upper-left vertex is a sample-point} \cr
				B_{i,j-1} + B_{i-1,j} -  B_{i-1,j-1}, \text{ Otherwise} 
			\end{cases}$	

		\item\label{whenToEvaluate} If $i,j < \nn+1$ and both $B_{ij} > B_{i,j-1} \vee B_{i-1,j}$, then evaluate $F_{ij} = F(x_i, y_j)$.

		\item If either $i$ or $j$ equals $\nn+1$ evaluate the marginal distribtion.

		\item\label{changes} Evaluate $|F_{ij} - B_{ab}|$ at $a \in \{i-1,i\}$ and $b \in \{j-1,j\}$ or only on $i-1$ and $j-1$ if on border. If it is bigger than $KS$ then update $KS$. If the update occured and the new $KS$ is such, that $F_{ij}\wedge B_{ij} > 1 - KS - \alpha$ then update $\jj$ to $j-1$.

		\item Increase $j$ by one. After the end of loop increase $i$ by one.

	\end{algorithm}
	\item Return $KS$.
\end{algorithm}
\end{minipage}
