\chapter{ Implementation }\label{Implementation}

In this chapter we will present the implementation of the \PTalgo, as described in Chapter \ref{theory}.

The analysis of the algorithm in all its complexity called for the development
of a more structurised template for carrying out numerical computations. The
\PTalgo, being an extension to the \MHalgo, does naturally share many similarities its structure. Both algorithms can have very
abstract formulations and can be carried out theoretically on any countably-
generated measurable space. Practical considerations would never go that far.
However, one cannot, in principle, discard the use of Stochastic Simulations in solving many real-life modelling problems such as, for instance, the Bayesian model selection, or Ising-type modelling of crystallic structures. The mulitude of potential applications calls for a reasonably flexible implemention of the \PTalgo. To overcome these problems, the object-oriented paradigm was used in this work, so that the \sspace\, and the \algo\, were two separate entities. 

Having realised the need for modularity, it seemed natural to take the whole idea one step further and develop a general template for Metropolis-Hastings-like simulations, going way beyond the idea of a task-specific computer programme. For there are other potential developments of the \MHalgo\, that are being used, all baring strong similarities in stuctures of these algorithms. Our template's goal was therefore to provide basic building blocks used for simulations, leaving the practitioners concentrate on the analysis. The template is codenamed \ssimul \, and that shall be the \RR\, package name when shipped.


\RR\, offers several implemented meta-structures that enable basic object oriented programming techniques. Among these there are the \textbf{S3} classes, \textbf{S4} classes, and the most recent Reference Classes. The implementation the template, was carried out using Reference Classes. This choice was dictated by several reasons. First, only \textbf{S4} classes and Reference Classes offer the possibility of basic type verification\footnote{Still far from the \Cpp\, standards though.}. This assured that no serious errors were introduced in the implementation phase and that users cannot provide absurd input for the algorithm. Moreover, Reference Classes are claimed to be the only implementation of objects in \RR\, that use passing arguments by reference, and a priori could saved some time in the calculations on unnecessary object copying\footnote{No serious differences in execution time were actually spoted when comparing both the functionally programmed prototype with the object-oriented final version, however.}. Finally, Reference Classes are considered to be highly compatible with \Cpp\, precompiled programmes being called from \RR. This is a clear advantage, as the future shipments of \ssimul\, are planned to be implemented in \Cpp, so that users can simulate faster that the whole programme was more memory efficient.   

\section{Division into objects}

To assure real modularity of our template, more divisions were proposed than only the one mentioned at the beginning of this chapter. The overall structure is represented in Figure \ref{objectStructure}.

We already mentioned the separation of the \algo\, from the \sspace. It is useful to think of the \algo\, as of a decision-maker. Its role is restricted to making decisions on acceptance or rejections of the proposals generated by the \sspace\, and then on giving orders to the \sspace\, to perform subsequent actions. The information on which the \algo\, bases its decisions depends on the points from the \sspace\, only indirectly, via the evaluations of densities\footnote{In the discrete case -- probability functions} at the proposal points. For in phase of the Random Walk the decision is based on the evaluation of 

\begin{equation*}
	\alpha_\beta(x_l , y_l) = 1 \wedge \frac{\pi^{\beta_l}(y_l)}{\pi^{\beta_l}(x_l)} = R\Big(\pi(y_l),\pi(x_l)\Big),
\end{equation*}

and in the Random Swap phase --- upon evaluation of one of the strategies, and additionally, in case of Strategy IV, of the quasi metric	

\begin{equation*}
	p_{ij}(x) = S\Big(\pi(x_i),\pi(x_j), \underbrace{\rho(x_i, x_j)}_\text{Random Swap only}\Big)
\end{equation*}

 A third candidate for an entity thus appears - namely an object whose methods would serve to measure some characteristic of the \sspace\, sample points for the use in the decision-making of the \algo. We called this entity the \measure. 

The very idea behind the \measure\, entity is that it should serve as a container for user-defined probability function or a density together with every additional data-structure required for its evaluation. Moreover, in case the user was interested in testing the \PTalgo's potential against some analytically tractable toy-example, or in case this example was easy to simulate using a simpler or potentially more efficient technique than \mcmc, the \measure\,
 entity would be the place to store any additional method of carrying out calculations regarding this particular distribution. For example, in our simulations (confront Chapter simulationsAndResults) we could  additionally provide efficient methods for quantile simulations and evaluates of the real distribuant. Implementations of these functions were collected as methods of the appropriately named instatiation of the \measure\, structure.  



\begin{figure}
	\begin{tikzpicture}
		[-,thick,%
		  every node/.style={shape=rectangle,inner sep=3pt,draw,thick}]
		\footnotesize
		\node {Simulations}
		  [sibling distance=4cm]
		  child {node {Algorithms}
		  	child {node {Metropolis-Hastings}
		  		child {node {Parallel Tempering}	
		  		}
		  	}	
		  }
		  child {node {State Spaces}
		    child {node {Real}
		    	child {node {Real Tempered}
		    	}
		    }
		  }
		  child {node {Target Measures}
		    [sibling distance=3cm]
		    child {node {Liang}
		    	child {node {Matteo}}
		    }
		    child {node {Unnormalised Densities}}
		  };
	\end{tikzpicture}
	\caption{Current operational entity-relations diagram.}\label{objectStructure}
\end{figure}

The reason for separating the \measure\, from the \sspace\, is again dictated by the modularity requirement. Totally different models can be modelled on the same \sspace\,, the only difference being the way the probability is assigned. It is therefore natural to define an entity whose task will be to serve as a warehouse for sample points, together with efficient methods or reading them, and in the end -- presenting the results to the user. These things, to some extent, can be done for any probability measure defined by the user. 

The Object Oriented paradigm gives the programmer also the tool of Inheritance. Its role is to assure no code getting copied when several programmes share the same structure and differ only at some minor implementation details. In our implementation we have noticed that the \PTalgo, being an extension to the \MH, differs only in the appearance of the swap stage and its methods, the Random Walk stage being almost the same\footnote{The difference being that it is called for several chains instead of one.}. The \PTalgo\, inherits from the \MH\, methods that are used for generating Random Walk proposals and updating the stored density evaluations. The \MH\, in its turn inherits fields from the \algo\, entity --- a virtual class, whose role is to store variables that can be used in any algorithm whatsoever, e.g. containing information on the user-supplied maximal number of iterations, or binary information on whether the calculations are done in the burn-in period, this being important for efficient storing of simulated points, or whether the simulation has been already carried out which is important for proper visualisation of the results. 

Some of the entities must contain pointers to other entities in order to be able to call methods of other entities when needed\footnote{Remember the \algo\, being compared to a decision maker.}. Because of certain Reference Classes limititions\footnote{As pointed out before, Reference Classes are relatively new, and, as such, poorly documented. With trial and error it has been checked, that this particular choice of entity-nesting simply works.}, the \algo\, has a pointer on the \sspace, which in its turn has a pointer on the \measure. Under different circumstances, different realisations of objects will appear as the \algo, the \sspace, and the \measure. For instantance, if the user wanted to carry out standard Metropolis-Hastings calculations on the three-dimensional euclidean space and check how it copes with the density function he provided, then the programme would have to first  initialise the \udensity\, being the instantiation of the \measure\, entity. Then, initialise the \rspace\, space, as an instatiation of the \sspace, and create a pointer on the \udensity. Finally, it would have to initialise the \MH\, being the operational \algo, and set the pointer on the previously initialised \rspace\, space. To automate this chain of initialisations, a controller entity was established under the name of \simulation. The \simulation\, greets the user, checks whether he is not interested in one of the preprogrammed examples of how different algorithms behave (compare to Chapter simulationsAndResults), and then instantiates all different entities using the user-provided informations. 

For reasons of user-friendliness, additional functions were written, whose names follow a simple convention. The name of the algorithm can be composed of up to two substings: the first substring being the name of the algorithm used, and the second - the name of some standard \sspace. The functions serve as wrappers for more general Reference Class constructor of the \simulation\, object. Continuing on the above-mentioned example, the user would simply had to call\footnote{The convention is known under the name of \textsc{camelCase}.}

\begin{lstlisting}
	Example <- metropolisReal(
		n = 1000,
		spaceDim = 2,
		targetDensity = userDefinedDensity
	)
\end{lstlisting}  
to initialise the simulation of the \MHalgo\, on the \rspace\, state space, with \textsc{userDefinedDensity} being an R-implemented unnormalised density function\footnote{In future versions \Cpp\, calls will be usable.}. If he was to use the \PTalgo\, approach, the initialisation code would be 

\begin{lstlisting}
	Example <- paralleltemperingRealtempered(
		n = 1000,
		spaceDim = 2,
		targetDensity = userDefinedDensity
	)
\end{lstlisting}

The presented object structure is far from final. Hopefully it will evolve in time, trying to match the needs of various users. We plan to the separate the swap distributions from the Parallel Tempering entity, giving the user the possibility to check swapping strategies tailored to his needs. Moreover, the existance of the \textsc{Real Tempered} entity stems from purely technical reasons of proper visualisation. The optimal solution would be to create a special controller class for visualisation that would take into account differences in the information presentation of results created through the use of different algorithms and provide some basic solutions.

To guarantee user-friendliness, the most common \sspace --- the multidimensional Euclidean space --- has been provided, so that the user can carry out computations specifying only the required minimum - the density function of the measure of interest. With future releases of the software, more standard State Spaces are scheduled for implemention as well, giving the practitioners experience the potentials and drawbacks of Metropolis-Hastings-like stochastic simulations.

\section{Functions and Methods}

A detailed enumeration of headers of the implemented methods together with brief description can be found in Appendix \ref{methodsDescribed}.

\section{Two-dimensional Колмогоров-Смирнов distance}

In order to check how different strategies approximate the toy-example distribution described in the next chapter, one had to settle for some kind of criterion. Recall that the general \ref{Problem} is to approximate an integral

$$\int_\Omega g \,\dip \approx \frac{1}{\nn}\sum_{i = 1}^\nn g(X^{[i]}). $$

One of the principal results in the Ergodic Theory is that one should observe weak convergence of empirical measures to the \measure\,$\Pi$ on almost all trajectories. The sample measures are simply taken to be

$$ \Pi_\nn(A) \equiv \frac{1}{\nn}\sum_{i = 1}^\nn \iii_A (X^{[i]}),$$ 
for $A \in \mathfrak{F}$. The weak convergence means that for any function $f: \Omega \mapsto \real$ bounded and continous and a fixed $\omega \in \Omega$ one observes 

$$ \Pi_\nn f (\omega)\equiv \frac{1}{\nn}\sum_{i = 1}^\nn \iii_A \Big(X^{[i]}(\omega)\Big) f\Big(X^{[i]}(\omega)\Big) \overset{\nn\rightarrow \infty}{\xrightarrow{\hspace*{1.5cm}}} \int_\Omega f(x) \Pi( \mathrm{d}\,x ),$$
and we denote it by $\Pi_\nn \rightharpoonup \Pi$. The above-mentioned result is simply

$$ \prob \Big( \Pi_\nn \rightharpoonup \Pi \Big) = 1.$$

If so, it is obvious that if the theorem holds, then on almost all trajectories one would also observe 

\begin{equation*}\label{KS definition}
	\prob\Biggl(\underset{x \in \real^\kk}{\sup}\Big| F(x) - \hat{F}_\nn (x)\Big| \rightarrow 0\Biggl) = 1,
\end{equation*}

where \Fecdf\, is the Empirical Cumulative Distribution Function --- \ecdf. If so, then if the values of the trajectory-based \ecdf\, were far away from the true distribuant, the approximation would be of poor quality. Therefore, if one is to compare different algorithms in their task to solve the Problem, as we do when applying different Swap Strategies, then a natural measure for such comparison is exactly the evaluation of the Колмогоров-Смирнов distance

$$ d_{KS}\Big( F, \hat{F}_\nn (\omega) \Big) \equiv \underset{x \in \real^\kk}{\sup}\Big| F(x) - \hat{F}_\nn (x)(\omega)\Big|.$$

But since it is trajectory-dependent, then a better way would be to carry out several hundreds of simulations and check for the differences in the mean values of the Колмогоров-Смирнов distance and its variance and use these quantities as proxies for our test. This is of course a heuristics, for the reasoning to be precise one would have to consider if the process has the huge bulk of its probability centered around the mean function.  

\todo{Importance of the KS in evaluating the integrals - the Vitali spaces.}  

The principal problem with the KS test is that it is computationally expensive and somewhat unpopular to carry out in the multidimensional set-up. Because of the lack of appropriate \RR-package, the whole procedure had to be reconsidered and implemented. 

Suppose we are given a simulation with $\tilde{\nn}$ iterations after the burn-in period. It often happens that the algorithm stays for more than one iteration in certain sample points\footnote{In fact we can easily control the point sejour-time due to the random walk by manipulating the proposal step's distribution parameters. In case of the kernel generated by the conditional normal distribution, it suffices to diminish the entries of the covariance matrix.}. We count the occurences of that points and then normalise them dividing them by $\tilde{\nn}$ and call the result {\it a charge}. Thus, we are given a matrix with sample coordinates and their charges. If the target measure $\pi$ is absolutely continous with respect to the Lebesgue measure, then the points generated by accepted proposals should all differ on both of their coordinates, the measure of any submanifold of $\real^\kk$ being zero. If that is not the case, then we simply face the problem of computer's finite arithmetics and could solve it by a longer binary representation of the real numbers. In practice that is an extremely unlikely event and we will neglect it. Therefore we will from now on focus on points with distinct coordinates and refer to the number of thereof by $\nn$. The world {\it sample} will also be applied only to the uppermentioned points. 

The computations of the \KS might be computationally expensive, requiring, in pesimistic case, evaluation of the true Cumulative Distribution Function $F$ in approximately $\frac{\nn^\kk}{\kk!}$ points\footnote{The exact number being equal to number of points on the intersection of a $\kk+1$ dimensional simplex with the coordinates summing to $\nn$, and the $\mathbb{Z}^{\kk+1}$ lattice.}. To see that, let us focus in all of this section on the bivariate case\footnote{\dots the generalisation to higher dimension being straighforward.}.   


\begin{figure}
	\centering \includegraphics[scale=1]{./img/KS1.eps}
	\caption{Sample points and level sets of an examplary \ecdf.}\label{simpleEcdf}
\end{figure}


Observe, that the level sets of any \ecdf\, are uniquely determined by the sample points ( as $A, B, C$ and $D$ in Figure \ref{simpleEcdf} ) and points generated from them by taking the vectorised maximum

$$ 
	\begin{bmatrix} a_1 \cr a_2 \end{bmatrix} \vee 
	\begin{bmatrix} b_1 \cr b_2 \end{bmatrix} = 
	\begin{bmatrix} a_1 \vee b_1 \cr a_2 \vee b_2 \end{bmatrix},
$$ 
such as $E$ in Figure \ref{simpleEcdf}.  

One of the easiest way of establishing the values on the level sets of \Fecdf\, is by considering a square, $(\nn+2)\times(\nn+2)$ matrix $B$, as in Figure \ref{spaceDivision}. The entries of $B$ correspond to values of \Fecdf\, on certain rectangles. Take the set of all $x$-coordinates of sample-points, $\Phi_x$, and the set of all $y$-coordinates of sample-points, $\Phi_y$. Add to them the coordinates of two dummy points: $(x_0, y_0)$ and $(x_{\nn+1}, y_{\nn+1})$ We enumerate points of these sets in the ascending order, $\Phi_x = \{ x_0 < x_1 < \dots <x_\nn < x_{\nn+1} \}$ and $\Phi_y = \{ y_0 < y_1 < \dots <y_\nn < y_{\nn+1} \}$. Then, the \Fecdf\, function is constant on rectangles 

$$R_{ij} = [x_i, x_{i+1})\times[y_j, y_{j+1}),$$ 
where $i,j \in \{0, 1, \dots, \nn+1 \}$. It is easy to realise now, that the number of level sets depends on the number of sample points and points generated by them using the vectorised maximum, $\hat{\nn} = \# \{ z : \exists x,y \in \text{Sample} x,y \not=z \text{ and } z = x \vee y \}$. This number is easily seen to be the greatest if all sample points could be arranged so that their $x$-coordinates strictly increase and $y$-coordinates strictly decrease.  

To actually derive the \KS\, we must assume that we can evaluate not only the true \cdf\, $F$, but also its two marginals\footnote{Limits of all the possible subsets of coordinates in infinity.}. Similarly to its univariate counterpart, the \KS\, can be calculated on only a finite number of points. Consider any rectangle. \Fecdf\, is constant on it. So the \KS depends solely on the evaluations of $F$ on that set. But $F$ is a distribuant - a function monotone in all its arguments, and so the maximum distance from \Fecdf\, can be attained only on the vertices of the rectangle. On the border rectangles ( $R_{i,\nn+1}$ and $R_{\nn+1,j}$ ) the evaluation simplifies to vertices adjacent to $R_{ij}$ rectangles and it is there, where we have to evaluate the marginals instead of $F$. \Fecdf\, is equal to zero on rectangles on the southern and western extremities. Thus we put $B_{i0} = B_{0j} = 0$. 

\begin{figure}
	\centering \includegraphics[scale=1]{./img/KS2.eps}
	\caption{Relation between $B$ matrix and \ecdf\, level sets.}\label{spaceDivision}
\end{figure}


The rest of the algorithm is based on the dynamic programming approach introduced by \citet*{NiVingron} for comparing lists of changes in gene expression under different scoring regimes. The overall algorithm proceeds as follows:

\begin{Algo}[2D-ECDF]
	\
	\begin{algorithmic}[1]
		\State $KS := 0$ 
		\State $\jj = \nn+1$
	 	\State Read $\alpha$
		\State Prepare $\Phi_x$ and $\Phi_y$. 	
		\While{$i \in \{1, \dots, \nn+1\}$}
			\While{$j \in \{ 1, \dots, \jj \}$}\label{jjCut}
				\State \,$B_{ij} \gets 
				\begin{cases} 
					B_{i-1,j-1} + \text{charge}, \text{ $R_{ij}$'s upper-left vertex contains a sample-point} \cr
					B_{i,j-1} + B_{i-1,j} -  B_{i-1,j-1}, \text{ otherwise.} 
				\end{cases}$
				\If{$i,j < \nn+1$ and $B_{ij} > B_{i,j-1} \vee B_{i-1,j}$} 
					\State $F_{ij} \gets F(x_i, y_j)$\label{whenToEvaluate}
				\ElsIf{$j=\nn+1$}
					\State $F_{ij} \gets F(x_i,\infty)$
				\ElsIf{$i=\nn+1$}
					\State $F_{ij} \gets F(\infty, y_j)$
				\EndIf
				\For{$a \in \{i-1,i\}$ and $b \in \{j-1,j\}$}
					\State $A \gets |F_{ij} - B_{ab}|$\label{changes}
					\If{$A > KS$} 
						\State $KS \gets A$
					\EndIf
					\If{$F_{ij}\wedge B_{ij} > 1 - KS - \alpha$}\label{CutOffCriterion}
						\State $\jj \gets j-1$\label{noUseToCalculateMore}
					\EndIf
				\EndFor
				\State $j \gets j+1$
			\EndWhile
			\State $i \gets i+1$
		\EndWhile
	\end{algorithmic}
\end{Algo}



Important changes with respect to \cite{NiVingron} can be seen in line \ref{changes} and the introduction of two extra variables $\jj$ and $\alpha$ in lines \ref{jjCut}, \ref{noUseToCalculateMore}, and \ref{CutOffCriterion}. The introduction of $\jj$ stems from the observation, that if both $F$ and \Fecdf\, are already in the interval $(1-KS,1]$, then any further evaluations of $KS$ to the East and North from the grid point under study could not result in a bigger difference then the one already observed. Parameter $\alpha$ enlarges that band further to $(1-KS-\alpha,1]$, so that the true \KS will not be larger than by $\alpha$ from the value already established. We have also noted that it is not important to evaluate the real distribuant at every point of the matrix $B$. Rather than that, we note that because $F$ is a distribuant, evaluations are required on South-Eastern edges of the level sets, such as points $A, B, C, D$, and $E$ on Figure \ref{spaceDivision}. That is the reason behind code in line \ref{whenToEvaluate}. Other changes seem to be of minor importance and result from discretisation of the {\it a priori} continous problem. 


Our impelementation of the \KS calculator might not the optimal one. As pointed out by \citet*{Jon} the problem of calculating values of an \ecdf\, only in the sample points could be solved in $\mathcal{O}(\nn \log( \nn ))$ iterations. The existence of an algorithm requiring only $\mathcal{O}(\hat{\nn} \log( \hat{\nn}  ))$ operations, interesting in its own sake, will be object of further research.
