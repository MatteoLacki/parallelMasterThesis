%!TEX program = <xelatex>
\documentclass{article}
\usepackage{mystyle}
\begin{document}

%!TEX root = <parallelTemperingMaster.tex>
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem statement}

Let $(\Omega, \mathfrak{F})$ be a measurable space. $\Omega$ --- the \sspace, a subset of a Polish space, and $\mathfrak{F}$ - the Borel subsets, hopefully countably-generated. Finally, denote the unit interval by $\mathcal{I} = [0,1]$. 

\begin{Problem}
	\item{\label{Problem}
		Given a distribution $\Pi: \mathfrak{F} \mapsto \mathcal{I}$
		\begin{Problem}
		 	\item absolutely continuous with respect to some measure on $\Omega$, with a density $\pi$ that we know to evaluate 
		 	\item density $\pi$ known up to its proportionality factor
		 	\item \label{multiModality}density $\pi$ being multi-modal
		\end{Problem} 
		draw a sample $\{ X_1, \dots, X_\nn \}$ from $\Pi$ so that for any function $g \in \mathbb{L}^1(\Pi)$ one can approximate well the integrals with empirical means
		$$\int_\Omega g \,\dip \approx \frac{1}{\nn}\sum_{i = 1}^\nn g(X^{[i]}). $$ 
	}
\end{Problem}

The rather abstract setting might seem to be an overshot, because, as the reader suspects, the most obvious and natural choice would be that of $\Omega = \real^\nn$ and $\mathfrak{F}$ the corresponding Borel sets. That, however, is not always the case, for very often one encounters some discrete spaces as well. Moreover, the abstract setting underlines the modularity of the implementation of the algorithm itself, being described in CHAPTER.

Had it been that $\pi$ was not multi-modal then by using the usual Metropolis-Hasting algorithm one could have easily solved the \ref{Problem}. Adding condition \ref{multiModality} however makes the use of that algorithm problematic because of its localness --- a phenomenon visualised in CHAPTER.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{The \PT} 
The Parallel Tempering algorithm is based on the idea of extending our attention to product $(\Omega^\lll, \mathfrak{F}^{\otimes \lll}, \pi_\beta)$ and equiping it with a specific Markov chain\footnote{Here $\mathfrak{F}^{\otimes \lll} \equiv \underbrace{\mathfrak{F} \otimes \dots \otimes \mathfrak{F}}_{\text{L times}}$ is the usual product sigma algebra.}. The main idea is that on each copy of $\Omega$ the corresponding chain will encounter a slightly modified version of the density of interest $\pi$. In the usual \PT\, one settles for 

\begin{assumptions}
	\item $\pi_\beta \propto \pi^{\beta_1} \times \dots \times \pi^{\beta_\lll},$\label{product form}
\end{assumptions}	

where $\beta = (\beta_1 , \dots , \beta_\lll)$ are such that $1 = \beta_1 > \dots > \beta_\lll > 0$. Their inversions, $T_i = \frac{1}{\beta_i}$, called temperatures, do form therefore a growing sequence of numbers. This naming convention finds its origin in the use of the \PT\, in statistical physics where the $T_i$ parameters had a direct interpretation. 

We do not assume to know the normalising constants for each $\pi^{\beta_l}$ --- we might even not know it for $\pi$ itself.

A Markov Chain\footnote{For the algorithm step numbering we shall henceforth use the square brackets notation.} $X \equiv \{ X^{[k]}\}_{k \geq 0}$ is then constructed on $\Omega^L$. Its construction will make use of two kernels that will make explicit use of $\pi^\beta_l$: one, the standard \randomWalk, will be responsible for the exploration of the \sspace, the other , called the \swapStep, for communication between different subchains, as $X$ can also be thought of as a collection of coordinate subchains 
$$X^{[k]} = (X_1^{[k]}, \dots, X_\lll^{[k]}).$$
Observe that the first one corresponds to the solution of the \ref{Problem}.

The main idea behind the construction of chain $X$ is that the subchains operating on tempered versions of $\pi$ influence the subchains linked with lower temperatures by offering them their own accepted proposals. This should be highly beneficial, as more tempered chains will encounter less problems in accepting proposals from regions where the first chain would hardly ever stray.  

To make this approach work at all we are still bound to make some other assumptions. 

\begin{assumptions}[resume]
	\item The starting points $X^{[0]}$ are selected so that $\pi(X_l^{[0]}) > 0$ for $l \in \{1,\dots,\lll\}$.
\end{assumptions}
	
Obviously, what follows is that $\pi(X_l^{[0]})^\beta_k > 0$ as well. It is important, for otherwise, what will be demonstrated in the following sections, the algorithm would be dividing by $0$ and break. 

The choice of modifying $\pi$ by exponentiation, besides its simplicity\footnote{For in general one could settle for some other kind of homotopic deformation.}, has considerable advantages in the analysis of the algorithm. For in the \randomWalk\,phase of the algorithm's step suppose a proposal $y$ generated by the $l^\text{th}$ chain from a region with less $\Pi$ probability on it in comparison to the position $x$ occupied in the last step, meaning $\pi(y) < \pi(x)$. Then

$$ 
	\alpha_{\beta_l}(x,y) =  1 \wedge \Biggl(\frac{\pi(y)}{\pi(x)}\Biggl)^{\beta_l} = 
	\Biggl(\frac{\pi(y)}{\pi(x)}\Biggl)^{\beta_l} > \frac{\pi(y)}{\pi(x)} = 1 \wedge \frac{\pi(y)}{\pi(x)} = \alpha_{\beta_1}(x,y),
$$

meaning that the probability of accepting such a proposal is larger on the more tempered chains. If on the other hand $\pi(y) > \pi(x)$, then this probability remains the same, $\alpha_{\beta_1}(x,y) = \alpha_{\beta_k}(x,y)$. This assures that more tempered chains can make longer excursions into regions not so much $\Pi$-probable. However, observe that 


\begin{itemize}	
	\item If we suppose that our distribution is multimodial, then simulating $\pi^{\beta_k}$ should genuinely enlarge the regions from which the proposals would get accepted. 
	\item However, if $\pi$'s modes are separated by regions as vast as to contain most of the proposal's probability, and yet bearing some insignificant amount of $\pi$'s mass, then the algorithm will stay local. This could be the case of modes separated by vast regions of $\pi$-measure zero. 
	\item Since the simulations will be carried out in the computer's finite arithmetic, it's possible that the mass of certain regions will be rounded down to zero, e.g. where the density is below the smallest representable number.  
\end{itemize}  




	\subsection*{Solution's Details}

The update mechanism consists of two stages. Assume that the algorithm has reached $n^\text{th}$ step, $X^{[n]}$. Following notation used in \citet*{BM1} we introduce two kernels that act subsequently on the MC

$$X^{[n-1]} \overset{\swap}{\rightarrow} \widetilde{X}^{[n-1]} \overset{\metro}{\rightarrow} X^{[n]}.$$

We shall refer to $\swap$ and $\metro$ as to the swap kernel and the random-walk kernel respectively.



	\subsubsection*{The random-walk kernel}
	
Assume $A_i \in \mathfrak{F}$ and $x \in \Omega^L$. Then $$\metro (x, A_1 \times \dots \times A_L) = \prod_{l=1}^L \metrobis{l}(x_l, A_l),$$
where $\metrobis{l}(x_l, A_l)$ corresponds to standard Metropolis kernel, as in \citet*{CharlesJ.Geyer}. 

In more detail 

$$\metrobis{l}(x_l, A_l) \equiv \int_A \alpha_{\beta_l} (x_l, y_l) q_{\Sigma_l} (y_l - x_l) \mathrm{d }y_l + \delta_x (A) \int [1 - \alpha_{\beta_l} (x_l, y_l)] q_{\Sigma_l} (y_l - x_l) \mathrm{d }y_l ,$$
the probability of accepting the update being precisely 

\begin{equation}\label{acceptance prob}
	\alpha_{\beta_l} (x_l, y_l)  \equiv 1 \wedge \frac{\pi^{\beta_l}(y_l)}{\pi^{\beta_l}(x_l)}.
\end{equation}


In the above equations the proposal distribution being used, following notation from \cite{CharlesJ.Geyer}, is $q(x_l,y_l) = q_{\Sigma_l} (y_l - x_l)$, where $q_{\Sigma_l}$ is the density of $\mathcal{N}(0, \Sigma_l)$. 

The simple form of Eq. \ref{acceptance prob} stems from proposal distribution symmetry, $$q(x_l,y_l) = q(y_l,x_l).$$

As indicated in \cite{BM2}, the execution of $\metro$ requires separate simulation of $\metrobis{l}$ for each of the coordinate chains,  $\widetilde{X}^{[n-1]}_l$.


	\subsection*{Metropolis-Hastings-Green algorithm.}

The ideas behind the random-walk kernel were straightforward. Before passing to the detailed description of the swap kernel, we give here a brief account of how the MHG algorithm works. The description together with the notation are taken from \cite{CharlesJ.Geyer}, pages 79-81. The introduction of this notation will be of considerable help in the understanding of the next section. 

The algorithm aims at generating samples from any distribution $\rho$ on measurable space,

	$$(\Gamma, \mathfrak{G}),$$

where $\Gamma \subset \mathbb{R}^n$ and $\mathfrak{G}$ is the appropriate borel $\sigma$-algebra. We suppose that we know $\rho$ up to a constant, so that $\rho = \frac{\eta}{\eta(\omega)}$, where $\eta$ is any finite real measure \footnote{In order to avoid confusion, in this section we use different notation for spaces and distributions than it was the case before. In the forthcoming applications, we will take $\Gamma$ to be $\Omega^L$, $\rho$ equal to $\pi_\beta$ and so on \dots}, 

	$$\rho: \mathfrak{G} \mapsto \mathbb{R}_{+}.$$  

To generalise the notion of Hasting's ratio, $R(x,y)$, one introduces first a suitable measure $\mu$ on the measurable space 

	$$(\Gamma^2, \mathfrak{G}^{\otimes 2}),$$ 

precisely where we assume our current-state $x$ and proposal $y$ to live together. Since our understanding of $y$ is that it is some value conditioned on the current-state $x$, the natural choice for $\mu$ is that of a regular conditional measure

	$$\mu(\mathrm{d }x, \mathrm{d }y)   \equiv \eta (\mathrm{d} x) Q(x, \mathrm{d } y),$$

where $Q(x,A)$ is our proposal kernel, possibly substochastic.
 
 
We also need a coordinate-swapping mapping $\phi: \Gamma^2 \mapsto \Gamma^2$ given by $$\phi(x,y) = (y,x),$$ which is obviously measurable, being continuous. Our understanding of the Green's ratio is now that of

	$$R(x,y) \equiv	\frac{\mathrm{d }(\mu \circ \phi^{-1})}{\mathrm{d }\mu} (x,y) = \frac{\rho(\mathrm{d y} )Q(y, \mathrm{d }x)}{\rho(\mathrm{d }x) Q(x, \mathrm{d }y)},$$

where $\phi^{-1}$ is the counter-image function related to $\phi$ and the derivative is understood in the Radon-Nikodym sense. 

Since we want to use the MHG algorithm with the state-dependent mixing, we shall equip both $Q$ and $R$ with additional indices $k$ that belong to $\mathcal{K}$, a set\footnote{Which can be infinite. In that case the forthcoming sums should be understood as integrals with respect to the counting measure.}.  

We require that

\begin{itemize}
	\item $Q_k (x,S)$ is known for all $k$,
	\item $\forall_{x \in \Gamma} \underset{ k \in \mathcal{K}}{\sum} Q_k (x,S) \leq 1$,
	\item For all $k \in \mathcal{K}$ $$R_k (x,y_k) \equiv \frac{\rho(\mathrm{d } y_k)Q_k (y_k, \mathrm{d }x)}{\rho(\mathrm{d } x)Q_k (x, \mathrm{d }y_k)}$$ is known ans possible to evaluate for any $x$ and $y_k$ \footnote{We introduce an extra index in $y_k$ to underline the potential difference between proposals under different kernels $Q_k$.},
	\item For each $x$ and $k$ its possible to draw from $$P_k(x, \circ) \equiv \frac{Q_k (x, \circ)}{Q_k (x, \Gamma)}$$
\end{itemize}

If these requirements are fulfilled, then the MHG update goes as follows

\begin{algorithm}
\item Simulate random index $K$ with probability $\mathbb{P}( K = k ) = Q_k (x,\Gamma)$.
\item[] With probability $1-\underset{k \in \mathcal{K}}{\sum} Q_k (x,\Gamma)$ skip the remaining steps and stay at $x$.
\item Simulate $Y \sim P_K (x, \circ)$.
\item Calculate $R_K (x,Y)$.
\item Accept $Y$ with probability $1\wedge R_K (x,Y)$.
\end{algorithm}

The kernel of the MHG update is of the following form

	$$P(x,A) \equiv \underset{k \in \mathcal{K}}{\sum} \int_A \alpha_k (x,y_k) Q_k(x, \mathrm{d }y_k) + \delta_x (A) \Big(1 - \underset{k \in \mathcal{K}}{\sum} \int_{\Omega} \alpha(x,y_k) Q_k(x,\mathrm{d }y_k) \Big), $$

where $\alpha(x,y_k) \equiv 1\wedge R_K (x,y_k)$. Note that this kernel is stochastic. According to the general theory, as stated in \cite{CharlesJ.Geyer}, the MHG update is reversible with respect to $\rho$. 



	\subsection*{The swap kernels}

Now we address the question of how to interlace the independent chains generated with kernel $\metro$. We want to swap the generated states so that chains with higher temperature influence the regions of lower temperatures. Being more precise, the swapping gives some possibility to the chains with lower temperatures to find themeselves in states that would otherwise be very unlikely to happen, the path leading to them being very unlikely to occur - these paths are rendered more probable for the chains with higher temperature. The swaps are made at random so that the probability of swaps takes into account some properties of $\pi$ evaluated on the last points from all the chains. 

Before passing to detailed description of different ways the swaps might be made, which we call \strats, let us first describe the properties of the swap kernel $\swap$ common to all \strats. In all cases, we rely on the following assumption

\begin{assumptions}[resume]
	\item At one step of the algorithm, there will be only one possible swap between a chosen at random pair of coordinates.
\end{assumptions}

The restriction here is not so much in the number of swaps per step. What seems to be more relevant is rather the constraint to a particular subset of the group of all permutations --- for one could certainly envisage that any random permutation could take place. However, it simplifies the interpretation of \strats\, that are to be described. 

Let us denote the swap operator by $S_{ij}$, so that 
$$S_{ij} x = (x_1, \dots, x_{i-1}, x_j, x_{i+1}, \dots, x_{j-1}, x_i, x_{j+1}, \dots, x_L).$$ 
We require $\swap(x, \circ )$ to be a measure concentrated on the set of all possible pairs of swaps between coordinates of $x = (x_1, \dots, x_L)$, i.e. on the set $\mathfrak{S}_x \equiv \{ S_{ij}x : i < j  \}$. We note that the proposal measure for the swap kernel $\mathcal{Q}$ is of the following form 

\begin{equation*}
	\mathcal{Q}(x, A) \equiv \underset{i < j}{\sum} p_{ij}(x) \mathbb{I}_A (S_{ij} x)
\end{equation*}	 

Then, the swap kernel would be of the following form, for any $x \in \Omega^L$ and $A$

\begin{equation*}
	\swap(x,A) = \int_{A} \alpha_\text{swap} (x,y) \mathcal{Q}(x, \mathrm{d}\,y) + r(x) \mathcal{I}(x,A)
\end{equation*}	

where $r(x) = 1 - \underset{\Omega^L}{\int} \alpha_\text{swap} (x,y) \mathcal{Q}(x, \mathrm{d}\,y) $. Plugging $\mathcal{Q}$ permits us to write

\begin{align*}
	\begin{split}
		\int_{A} \alpha_\text{swap} (x,y) \mathcal{Q}(x, \mathrm{d}\,y) &= \underset{ i < j}{\sum} p_{ij}(x) \int \alpha_\text{swap} (x,y) \mathbb{I}_A (y) \delta_{S_{ij}x}(\mathrm{d}\, y) \\ &= \underset{ i < j}{\sum} p_{ij}(x) \alpha_\text{swap} (x, S_{ij}x) \mathbb{I}_A(S_{ij} x),
	\end{split}
\end{align*}	

so that finally

\begin{equation*}
	\swap(x,A) = \underset{ i < j}{\sum} p_{ij}(x) \alpha_\text{swap} (x, S_{ij}x) \mathbb{I}_A(S_{ij} x) + \Big( 1 - \underset{ i < j}{\sum} p_{ij}(x) \alpha_\text{swap} (x, S_{ij}x)\Big) \mathcal{I}(x,A),
\end{equation*}	

where 

	$$\alpha_\text{swap}(x,y) = \frac{\pi_\beta( \mathrm{d}\, y ) \mathcal{Q}(y, \mathrm{d}\,x)}{\pi_\beta( \mathrm{d}\, x ) \mathcal{Q}(x, \mathrm{d}\,y)} \wedge 1$$

is the Radon-Nikodym derivative of two measures. It's calculated with respect to measure $ \mu (\mathrm{d}\,x, \mathrm{d}\,y) \equiv \pi(\mathrm{d}\,x) \mathcal{Q}(x, \mathrm{d}\,y)$. The measure that is being differentiated is $\mu$ composed with the exchange coordinates operation, $R(x,y) = (y,x)$\footnote{Which is obviously measurable in the appropriate sense.}. So finally $\alpha_\text{swap} \equiv \frac{\mathrm{d}\, \mu \circ R^{-1}}{\mathrm{d}\, \mu}$.

Observe that thanks to the proposal's support finiteness we actually get 

	$$\alpha_\text{swap}(x,y) =  \frac{\pi_\beta( y ) \mathcal{Q}(y, x)}{\pi_\beta( x ) \mathcal{Q}(x, y)} \wedge 1.$$


Now, since $y \in \mathfrak{S}_x $, the tagetted $\pi_\beta$ is a tensor of measures, $Q(x, S_{ij} x) = p_{ij}(x)$, and $Q( S_{ij} x, x) = p_{ij}(S_{ij}x)$ \footnote{Here we pass from measure notation to probability function notation, $\mathcal{Q}(x, \{S_{ij}x \}) = \mathcal{Q}(x, S_{ij}x)$. We can do it because $\mathcal{Q}$ is purely atomic given $x$.}, we get finally 

\begin{equation*}
	\alpha_\text{swap}(x,S_{ij} x) = \Big[  \Big(\frac{\pi(x_j)}{\pi(x_i)} \Big)^{\beta_i - \beta_j}  \frac{ p_{ij}(S_{ij} x )}{ p_{ij}( x ) }\Big] \wedge 1
\end{equation*}	

In our computer simulations several swapping strategies have been tested. For instance,

\begin{strategy}
	\item 
		$
			p_{ij}(x) \propto 
			\frac{\pi (x_j)}{\pi( x_i )} \wedge \frac{\pi (x_i)}{\pi( x_j )} = 
			\exp \Big( - \big| \log ( \pi(x_j) ) - \log ( \pi(x_i) ) \big| \Big).
		$\label{strat1} 
\end{strategy}

In this particular the proportionality is in fact equality, because the above expression is transposition symmetric and this considerably simplifies the statistical sum analysis. In general, we want $p_{ij}(x) \propto h(x_i,x_j)$ for some function $h$. Thus,

\begin{equation*}
	\frac{p_{kl}(S_{kl}x)}{p_{kl}(x)} = \frac{\frac{h(x_l,x_k)}{A(\tilde{x}_{kl})+f(x_k)+g(x_l)+h(x_l,x_k)}}{
	\frac{h(x_k,x_l)}{A(\tilde{x}_{kl})+f(x_l)+g(x_k)+h(x_k,x_l)}
	} = 
	\underbrace{\frac{h(x_l,x_k)}{h(x_k,x_l)}}_\text{'direct effect'} 
	\times 
	\underbrace{\frac{A(\tilde{x}_{kl})+f(x_l)+g(x_k)+h(x_k,x_l)}{A(\tilde{x}_{kl})+f(x_k)+g(x_l)+h(x_l,x_k)}}_\text{'distribution effect'},	 	 	
\end{equation*}  
where $\tilde{x}_{kl}$ is $x$ without its $k^\text{th}$ and $l^\text{th}$ coordinates, $f(x_l) = \sum_{i<l, i\not=k} h(x_i, x_l)$, and $g(x_k) = \sum_{j>k, i\not=l}h(x_k, x_j)$. As we see it is fairly straightforward to compare the above expression to $1$ if the 'distribution effect' equals $1$, and difficult had it been the other case. To assure this happens, one can settle for a function $h$ that is transposition invariant, $h(x,y)=h(y,x)$. This is certainly the case of \ref{strat1}. In this strategy therefore we are promoting exactly swaps between coordinates whose proposals have relatively the same $\pi$ density values, i.e. $\pi (x_j) \approx \pi (x_i)$. 



\begin{strategy}[resume]
	\item 
		$
			p_{ij}(x) \propto 
			\frac{\pi (x_j)}{\pi (x_i)} \wedge 1 = 
			\exp \Big( - \big[ \log ( \pi(x_j) ) - \log ( \pi(x_j) ) \big]\Big) \wedge 1.$\label{strat2}
\end{strategy}

This strategy breaks the symmetry of the previous one rendering the evaluation of the effect on the $\alpha_\text{swap}$ challenging, if not impossible. The main idea behind this strategy was to  


The subsequent strategies are again functions of the fist strategy.

\begin{strategy}[resume]
	\item 
		$p_{ij}(x) \propto 
			\Big( \frac{\pi (x_j)}{\pi( x_i )} \wedge 
			\frac{\pi (	x_i)}{\pi( x_j )} \Big)^{|\beta_i - \beta_j|} = 
		\exp \Big( - |\beta_i - \beta_j| \times \big| \log ( \pi(x_j) ) - \log ( \pi(x_i) ) \big| \Big).$\label{strat3} 
\end{strategy}

This strategy permits us to soften a bit the requirement that $\pi(x_j) \approx \pi (x_i)$. This effect is strengthened for coordinates that are similarly tempered, i.e. where $\beta_i - \beta_j \approx 0$. Swaps between adjacent chains will be therefore more probable. 

One can also use a strategy that incorporates some distance measure between points, $\rho$ being any quasi-metric. \ref{strat4} is a good example of such procedure - the more distant the points, the less probable it is for them to get swapped.  

\begin{strategy}[resume]
	\item 
		$p_{ij}(x) \propto \Big( \frac{\pi (x_j)}{\pi( x_i )} \wedge \frac{\pi (x_i)}{\pi( x_j )} \Big)^\frac{|\beta_i - \beta_j|}{1 + \rho(x_i, x_j)} = \exp \Big( - \frac{|\beta_i - \beta_j| \times | \log ( \pi(x_j) ) - \log ( \pi(x_i) ) |}{{1 + \rho(x_i, x_j)}} \Big).$\label{strat4}
\end{strategy} 

Finally, for purposes of comparisons with \citet{BaragattiParallelTemperingWithEquiEnergyMoves}, we include also strategies that are \sspace\, independent, i.e. the probability of swaps does not depend on the evaluations of the density $\pi$ in the sample points. Such na\"ive strategies include 

\begin{strategy}[resume]
	\item $p_{ij} = \frac{2}{\lll (\lll - 1)}$\label{strat5}
\end{strategy}

and 

\begin{strategy}[resume]
	\item $p_{ij} = \frac{1}{\lll - 1} \ind{\{|i-j|=1\}}$\label{strat6}
\end{strategy}

amounting to choosing uniformly among all possible swaps and all neighbouring-in-temperature swap respectively. 












	\bibliographystyle{./bibliography/eccaNoNotes}
	\bibliography{./bibliography/myBooks}

\end{document}