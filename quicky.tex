%!TEX program = <xelatex>
\documentclass{article}
\usepackage{mystyle}
\begin{document}

%!TEX root = <parallelTemperingMaster.tex>
\section*{Problem statement}

\subsection*{Target}
Let $(\Omega, \mathfrak{F})$ be a measurable space, $\Omega$ - a subset of a Polish space, $\mathfrak{F}$ - Borel subsets, hopefully countably-generated, $\mathcal{I} = [0,1]$.

Suppose that we want to draw sample from a given distribution $ \pi: \mathfrak{F} \mapsto \mathcal{I}$. We assume that $\pi$ is absolutely continuous with respect to some measure on $\mathbb{R}^n$ and call $\pi$ its appropriate density function. We assume to know $\pi$ up to its proportionality factor. If $\pi$ is multi-modal then the usual Metropolis-Hasting-Green algorithm might get stuck in one of $\pi$'s modes and renders completely erroneous results.


\subsection*{Solution's Ideology} 
The Parallel Tempering algorithm is based on the idea of observing a Markov chain over the product space $$(\Omega^L, \mathfrak{F}^{\otimes L}, \pi_\beta)$$

where 

	$$\mathfrak{F}^{\otimes L} \equiv \underbrace{\mathfrak{F} \otimes \dots \otimes \mathfrak{F}}_{\text{$L$ times}},$$

is the usual product sigma algebra, and finally 

\begin{assumptions}
	\item $\pi_\beta \propto \pi^{\beta_1} \times \dots \times \pi^{\beta_L},$\label{product form}
\end{assumptions}	

where $\beta = (\beta_1 , \dots , \beta_L)$ are inverse temperatures s.t. $1 = \beta_1 > \dots > \beta_L > 0$, so that the temperatures are growing, $1 = T_1 < \dots < T_L < \infty$.

We do not assume to know the normalising constants for each $\pi^{\beta_l}$, since we don't know it for $\pi$ itself.

The Markov chain $X \equiv \{ X^{[k]}\}_{k \geq 0}$ has $\Omega^L$ for its state-space \footnote{For the algorithm step numbering we shall henceforth use the square brackets notation.}. Thus $$X^{[k]} = (X_1^{[k]}, \dots, X_L^{[k]}).$$

The idea behind the algorithm is that of simulating the MC $X$ so that its coordinates with higher temperatures influence the coordinates with lower temperatures. This influence should be beneficial because coordinates with higher temperatures are drawn from distributions that are "less multi-modial" in the sense that

\begin{assumptions}[resume]
	\item $\pi > 0$ on its support \dots
\end{assumptions}
	

\begin{itemize}	
	\item then $\pi^{\beta_k} > \pi$ for $k \geq 2$\dots
	\item so $\alpha_{\beta_1}(x,y) = 1 \wedge \frac{\pi(y)}{\pi(x)} = \frac{\pi(y)}{\pi(x)} <  (\frac{\pi(y)}{\pi(x)})^{\beta_k} = 1 \wedge (\frac{\pi(y)}{\pi(x)})^{\beta_k} = \alpha_{\beta_k}(x,y)$ if only $\pi(y) < \pi(x)$, and $\alpha_{\beta_1}(x,y) = \alpha_{\beta_k}(x,y)$ otherwise. Therefore the probability of accepting the proposal $y$ is significantly higher in case of the more tempered chains when $\frac{\pi(y)}{\pi(x)} < 1$.
	\item If we suppose that our distribution is multimodial, then simulating $\pi^{\beta_k}$ should genuinely enlarge the regions from which the proposals would get accepted. 
	\item However, if $\pi$'s modes are separated by regions as vast as to contain most of the proposal's probability, and yet bearing some insignificant amount of $\pi$'s mass, then the algorithm will stay local. This could be the case of modes separated by vast regions of $\pi$-measure zero. 
	\item Since the simulations will be carried out in the computer's finite arithmetic, it's possible that the mass of certain regions will be rounded down to zero, e.g. where the density is below the smallest representable number.  
\end{itemize}  




	\subsection*{Solution's Details}

The update mechanism consists of two stages. Assume that the algorithm has reached $n^\text{th}$ step, $X^{[n]}$. Following notation used in \citet*{BM1} we introduce two kernels that act subsequently on the MC

$$X^{[n-1]} \overset{\swap}{\rightarrow} \widetilde{X}^{[n-1]} \overset{\metro}{\rightarrow} X^{[n]}.$$

We shall refer to $\swap$ and $\metro$ as to the swap kernel and the random-walk kernel respectively.



	\subsubsection*{The random-walk kernel}
	
Assume $A_i \in \mathfrak{F}$ and $x \in \Omega^L$. Then $$\metro (x, A_1 \times \dots \times A_L) = \prod_{l=1}^L \metrobis{l}(x_l, A_l),$$
where $\metrobis{l}(x_l, A_l)$ corresponds to standard Metropolis kernel, as in \citet*{CharlesJ.Geyer}. 

In more detail 

$$\metrobis{l}(x_l, A_l) \equiv \int_A \alpha_{\beta_l} (x_l, y_l) q_{\Sigma_l} (y_l - x_l) \mathrm{d }y_l + \delta_x (A) \int [1 - \alpha_{\beta_l} (x_l, y_l)] q_{\Sigma_l} (y_l - x_l) \mathrm{d }y_l ,$$
the probability of accepting the update being precisely 

\begin{equation}\label{acceptance prob}
	\alpha_{\beta_l} (x_l, y_l)  \equiv 1 \wedge \frac{\pi^{\beta_l}(y_l)}{\pi^{\beta_l}(x_l)}.
\end{equation}


In the above equations the proposal distribution being used, following notation from \cite{CharlesJ.Geyer}, is $q(x_l,y_l) = q_{\Sigma_l} (y_l - x_l)$, where $q_{\Sigma_l}$ is the density of $\mathcal{N}(0, \Sigma_l)$. 

The simple form of Eq. \ref{acceptance prob} stems from proposal distribution symmetry, $$q(x_l,y_l) = q(y_l,x_l).$$

As indicated in \cite{BM2}, the execution of $\metro$ requires separate simulation of $\metrobis{l}$ for each of the coordinate chains,  $\widetilde{X}^{[n-1]}_l$.


	\subsection*{Metropolis-Hastings-Green algorithm.}

The ideas behind the random-walk kernel were straightforward. Before passing to the detailed description of the swap kernel, we give here a brief account of how the MHG algorithm works. The description together with the notation are taken from \cite{CharlesJ.Geyer}, pages 79-81. The introduction of this notation will be of considerable help in the understanding of the next section. 

The algorithm aims at generating samples from any distribution $\rho$ on measurable space,

	$$(\Gamma, \mathfrak{G}),$$

where $\Gamma \subset \mathbb{R}^n$ and $\mathfrak{G}$ is the appropriate borel $\sigma$-algebra. We suppose that we know $\rho$ up to a constant, so that $\rho = \frac{\eta}{\eta(\omega)}$, where $\eta$ is any finite real measure \footnote{In order to avoid confusion, in this section we use different notation for spaces and distributions than it was the case before. In the forthcoming applications, we will take $\Gamma$ to be $\Omega^L$, $\rho$ equal to $\pi_\beta$ and so on \dots}, 

	$$\rho: \mathfrak{G} \mapsto \mathbb{R}_{+}.$$  

To generalise the notion of Hasting's ratio, $R(x,y)$, one introduces first a suitable measure $\mu$ on the measurable space 

	$$(\Gamma^2, \mathfrak{G}^{\otimes 2}),$$ 

precisely where we assume our current-state $x$ and proposal $y$ to live together. Since our understanding of $y$ is that it is some value conditioned on the current-state $x$, the natural choice for $\mu$ is that of a regular conditional measure

	$$\mu(\mathrm{d }x, \mathrm{d }y)   \equiv \eta (\mathrm{d} x) Q(x, \mathrm{d } y),$$

where $Q(x,A)$ is our proposal kernel, possibly substochastic.
 
 
We also need a coordinate-swapping mapping $\phi: \Gamma^2 \mapsto \Gamma^2$ given by $$\phi(x,y) = (y,x),$$ which is obviously measurable, being continuous. Our understanding of the Green's ratio is now that of

	$$R(x,y) \equiv	\frac{\mathrm{d }(\mu \circ \phi^{-1})}{\mathrm{d }\mu} (x,y) = \frac{\rho(\mathrm{d y} )Q(y, \mathrm{d }x)}{\rho(\mathrm{d }x) Q(x, \mathrm{d }y)},$$

where $\phi^{-1}$ is the counter-image function related to $\phi$ and the derivative is understood in the Radon-Nikodym sense. 

Since we want to use the MHG algorithm with the state-dependent mixing, we shall equip both $Q$ and $R$ with additional indices $k$ that belong to $\mathcal{K}$, a set\footnote{Which can be infinite. In that case the forthcoming sums should be understood as integrals with respect to the counting measure.}.  

We require that

\begin{itemize}
	\item $Q_k (x,S)$ is known for all $k$,
	\item $\forall_{x \in \Gamma} \underset{ k \in \mathcal{K}}{\sum} Q_k (x,S) \leq 1$,
	\item For all $k \in \mathcal{K}$ $$R_k (x,y_k) \equiv \frac{\rho(\mathrm{d } y_k)Q_k (y_k, \mathrm{d }x)}{\rho(\mathrm{d } x)Q_k (x, \mathrm{d }y_k)}$$ is known ans possible to evaluate for any $x$ and $y_k$ \footnote{We introduce an extra index in $y_k$ to underline the potential difference between proposals under different kernels $Q_k$.},
	\item For each $x$ and $k$ its possible to draw from $$P_k(x, \circ) \equiv \frac{Q_k (x, \circ)}{Q_k (x, \Gamma)}$$
\end{itemize}

If these requirements are fulfilled, then the MHG update goes as follows

\begin{algorithm}
\item Simulate random index $K$ with probability $\mathbb{P}( K = k ) = Q_k (x,\Gamma)$.
\item[] With probability $1-\underset{k \in \mathcal{K}}{\sum} Q_k (x,\Gamma)$ skip the remaining steps and stay at $x$.
\item Simulate $Y \sim P_K (x, \circ)$.
\item Calculate $R_K (x,Y)$.
\item Accept $Y$ with probability $1\wedge R_K (x,Y)$.
\end{algorithm}

The kernel of the MHG update is of the following form

	$$P(x,A) \equiv \underset{k \in \mathcal{K}}{\sum} \int_A \alpha_k (x,y_k) Q_k(x, \mathrm{d }y_k) + \delta_x (A) \Big(1 - \underset{k \in \mathcal{K}}{\sum} \int_{\Omega} \alpha(x,y_k) Q_k(x,\mathrm{d }y_k) \Big), $$

where $\alpha(x,y_k) \equiv 1\wedge R_K (x,y_k)$. Note that this kernel is stochastic. According to the general theory, as stated in \cite{CharlesJ.Geyer}, the MHG update is reversible with respect to $\rho$. 



	\subsection*{The swap kernels}

Now we address the question of how to interlace the independent chains generated with kernel $\metro$. We want to swap the generated states so that chains with higher temperature influence the regions of lower temperatures. Being more precise, the swapping gives some possibility to the chains with lower temperatures to find themeselves in states that would otherwise be very less probable to occur. The swaps will be random and the probability of swaps should take into account some properties of $\pi$. Here we propose several state-dependent strategies of how to promote certain swaps.

Let us first describe the swap kernel $\swap$. 

\begin{assumptions}[resume]
	\item At one step of the algorithm, there will be only one possible swap between a chosen at random pair of coordinates.
\end{assumptions}

Let $S_{ij} x = (x_1, \dots, x_{i-1}, x_j, x_{i+1}, \dots, x_{j-1}, x_i, x_{j+1}, \dots, x_L)$. We require $\swap(x, \circ )$ to be a measure concentrated on the set of all possible pairs of swaps between coordinates of $x = (x_1, \dots, x_L)$, i.e. on the set $\mathfrak{S}_x \equiv \{ S_{ij}x : i < j  \}$. Let's assume that the proposal measure for the swap kernel $\mathcal{Q}$ is of the following form 

\begin{equation*}
	\mathcal{Q}(x, A) \equiv \underset{i < j}{\sum} p_{ij}(x) \mathbb{I}_A (S_{ij} x)
\end{equation*}	 

Then, the swap kernel would be of the following form, for any $x \in \Omega^L$ and $A$

\begin{equation*}
	\swap(x,A) = \int_{A} \alpha_\text{swap} (x,y) \mathcal{Q}(x, \mathrm{d}\,y) + r(x) \mathcal{I}(x,A)
\end{equation*}	

where $r(x) = 1 - \underset{\Omega^L}{\int} \alpha_\text{swap} (x,y) \mathcal{Q}(x, \mathrm{d}\,y) $. Plugging $\mathcal{Q}$ permits us to write

\begin{align*}
	\begin{split}
		\int_{A} \alpha_\text{swap} (x,y) \mathcal{Q}(x, \mathrm{d}\,y) &= \underset{ i < j}{\sum} p_{ij}(x) \int \alpha_\text{swap} (x,y) \mathbb{I}_A (y) \delta_{S_{ij}x}(\mathrm{d}\, y) \\ &= \underset{ i < j}{\sum} p_{ij}(x) \alpha_\text{swap} (x, S_{ij}x) \mathbb{I}_A(S_{ij} x),
	\end{split}
\end{align*}	

so that finally

\begin{equation*}
	\swap(x,A) = \underset{ i < j}{\sum} p_{ij}(x) \alpha_\text{swap} (x, S_{ij}x) \mathbb{I}_A(S_{ij} x) + \Big( 1 - \underset{ i < j}{\sum} p_{ij}(x) \alpha_\text{swap} (x, S_{ij}x)\Big) \mathcal{I}(x,A),
\end{equation*}	

where 

	$$\alpha_\text{swap}(x,y) = \frac{\pi_\beta( \mathrm{d}\, y ) \mathcal{Q}(y, \mathrm{d}\,x)}{\pi_\beta( \mathrm{d}\, x ) \mathcal{Q}(x, \mathrm{d}\,y)} \wedge 1$$

is the Radon-Nikodym derivative of two measures. It's calculated with respect to measure $ \mu (\mathrm{d}\,x, \mathrm{d}\,y) \equiv \pi(\mathrm{d}\,x) \mathcal{Q}(x, \mathrm{d}\,y)$. The measure that is being differentiated is $\mu$ composed with the exchange coordinates operation, $R(x,y) = (y,x)$\footnote{Which is obviously measurable in the appropriate sense.}. So finally $\alpha_\text{swap} \equiv \frac{\mathrm{d}\, \mu \circ R^{-1}}{\mathrm{d}\, \mu}$.

Observe that thanks to the proposal's support finiteness we actually get 

	$$\alpha_\text{swap}(x,y) =  \frac{\pi_\beta( y ) \mathcal{Q}(y, x)}{\pi_\beta( x ) \mathcal{Q}(x, y)} \wedge 1.$$


Now, since $y \in \mathfrak{S}_x $, the tagetted $\pi_\beta$ is a tensor of measures, $Q(x, S_{ij} x) = p_{ij}(x)$, and $Q( S_{ij} x, x) = p_{ij}(S_{ij}x)$ \footnote{Here we pass from measure notation to probability function notation, $\mathcal{Q}(x, \{S_{ij}x \}) = \mathcal{Q}(x, S_{ij}x)$. We can do it because $\mathcal{Q}$ is purely atomic given $x$.}, we get finally 

\begin{equation*}
	\alpha_\text{swap}(x,S_{ij} x) = \Big[  \Big(\frac{\pi(x_j)}{\pi(x_i)} \Big)^{\beta_i - \beta_j}  \frac{ p_{ij}(S_{ij} x )}{ p_{ij}( x ) }\Big] \wedge 1
\end{equation*}	

In our computer simulations several swapping strategies have been tested. For instance,

\begin{strategy}
	\item 
		$
			p_{ij}(x) \propto 
			\frac{\pi (x_j)}{\pi( x_i )} \wedge \frac{\pi (x_i)}{\pi( x_j )} = 
			\exp \Big( - \big| \log ( \pi(x_j) ) - \log ( \pi(x_i) ) \big| \Big).
		$\label{strat1} 
\end{strategy}

In this particular the proportionality is in fact equality, because the above expression is transposition symmetric and this considerably simplifies the statistical sum analysis. In general, we want $p_{ij}(x) \propto h(x_i,x_j)$ for some function $h$. Thus,

\begin{equation*}
	\frac{p_{kl}(S_{kl}x)}{p_{kl}(x)} = \frac{\frac{h(x_l,x_k)}{A(\tilde{x}_{kl})+f(x_k)+g(x_l)+h(x_l,x_k)}}{
	\frac{h(x_k,x_l)}{A(\tilde{x}_{kl})+f(x_l)+g(x_k)+h(x_k,x_l)}
	} = 
	\underbrace{\frac{h(x_l,x_k)}{h(x_k,x_l)}}_\text{'direct effect'} 
	\times 
	\underbrace{\frac{A(\tilde{x}_{kl})+f(x_l)+g(x_k)+h(x_k,x_l)}{A(\tilde{x}_{kl})+f(x_k)+g(x_l)+h(x_l,x_k)}}_\text{'distribution effect'},	 	 	
\end{equation*}  
where $\tilde{x}_{kl}$ is $x$ without its $k^\text{th}$ and $l^\text{th}$ coordinates, $f(x_l) = \sum_{i<l, i\not=k} h(x_i, x_l)$, and $g(x_k) = \sum_{j>k, i\not=l}h(x_k, x_j)$. As we see it is fairly straightforward to compare the above expression to $1$ if the 'distribution effect' equals $1$, and difficult had it been the other case. To assure this happens, one can settle for a function $h$ that is transposition invariant, $h(x,y)=h(y,x)$. This is certainly the case of \ref{strat1}. In this strategy therefore we are promoting exactly swaps between coordinates whose proposals have relatively the same $\pi$ density values, i.e. $\pi (x_j) \approx \pi (x_i)$. 



\begin{strategy}[resume]
	\item 
		$
			p_{ij}(x) \propto 
			\frac{\pi (x_j)}{\pi (x_i)} \wedge 1 = 
			\exp \Big( - \big[ \log ( \pi(x_j) ) - \log ( \pi(x_j) ) \big]\Big) \wedge 1.$\label{strat2}
\end{strategy}

This strategy breaks the symmetry of the previous one rendering the evaluation of the effect on the $\alpha_\tetx{swap}$ challenging, if not impossible.  


The subsequent strategies are again functions of the fist strategy.

\begin{strategy}[resume]
	\item 
		$p_{ij} \propto 
			\Big( \frac{\pi (x_j)}{\pi( x_i )} \wedge 
			\frac{\pi (	x_i)}{\pi( x_j )} \Big)^{|\beta_i - \beta_j|} = 
		\exp \Big( - |\beta_i - \beta_j| \times \big| \log ( \pi(x_j) ) - \log ( \pi(x_i) ) \big| \Big).$\label{strat3} 
\end{strategy}

This strategy permits us to soften a bit the requirement that $\pi(x_j) \approx \pi (x_i)$. This effect is strengthened for coordinates that are similarly tempered, i.e. where $\beta_i - \beta_j \approx 0$. Swaps between adjacent chains will be therefore more probable. 

One can also use a strategy that incorporates some distance measure between points, $\rho$ being any quasi-metric. \ref{strat4} is a good example of such procedure - the more distant the points, the less probable it is for them to get swapped.  

\begin{strategy}[resume]
	\item 
		$p_{ij} \propto \Big( \frac{\pi (x_j)}{\pi( x_i )} \wedge \frac{\pi (x_i)}{\pi( x_j )} \Big)^\frac{|\beta_i - \beta_j|}{1 + \rho(x_i, x_j)} = \exp \Big( - \frac{|\beta_i - \beta_j| \times | \log ( \pi(x_j) ) - \log ( \pi(x_i) ) |}{{1 + \rho(x_i, x_j)}} \Big).$\label{strat4}
\end{strategy} 

Finally, for purposes of comparisons with \citet{BaragattiParallelTemperingWithEquiEnergyMoves}, we include also strategies that are \sspace\, independent, i.e. the probability of swaps does not depend on the evaluations of the density $\pi$ in the sample points. Such na\"ive strategies include 

\begin{strategy}[resume]
	\item $p_{ij} = \frac{2}{\lll (\lll - 1)}$\label{strat5}
\end{strategy}

and 

\begin{strategy}[resume]
	\item $p_{ij} = \frac{1}{\lll - 1} \ind{\{|i-j|=1\}}$\label{strat6}
\end{strategy}

amounting to choosing uniformly among all possible swaps and all neighbouring-in-temperature swap respectively. 












	\bibliographystyle{./bibliography/eccaNoNotes}
	\bibliography{./bibliography/myBooks}

\end{document}